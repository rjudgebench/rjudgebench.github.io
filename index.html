<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>R-Judge</title>
    <link rel="icon" href="assets/logo.png" type="image/icon type">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/nav.css">
    <script src="javascript/index.js" defer></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    <div id="nav">
        <div id="icon">
            <img src="assets/logo.png" id="nav-icon">
            <a class="nav-button" href="https://rjudgebench.github.io/"
                style="margin-left: 2px; font-size: 24px">R-Judge
            </a>
        </div>
        <div>
            <a class="nav-button" href="#home">Home</a>
            <a class="nav-button" href="#dataset">Dataset</a>
            <a class="nav-button" href="#method">Method</a>
            <a class="nav-button" href="#paper">Paper</a>
            <a class="nav-button" href="#code">Code</a>
            <a class="nav-button" href="#citation">Citation</a>
            <a class="nav-button" href="#contact">Contact</a>
            <a class="nav-button" href="leaderboard.html">Leaderboard</a>
            <a class="nav-button" href="explore.html">Explore</a>
        </div>
    </div>

    <!-- anchor for the home button -->
    <div id="home" style="position: absolute; top: 0;"></div>

    <!-- banner -->
    <div id="title">
        <div id="title-wrapper">
            <img src="assets/logo.png" id="title-icon">
            <p id="title-text">R-Judge</p>
        </div>
        <p id="subtitle-text">
            Benchmarking Safety Risk Awareness 
            <br>for LLM Agents
            <br><br>
            (2024)
        </p>
        <p id="title-padding-bottom">&nbsp;</p>
    </div>

    <!-- the main body of the page -->
    <!-- each section uses an empty div as an anchor -->
    <div id="body">
        <div class="section">
            <div id="about" class="anchor"></div>
            <h1>About R-Judge</h1>
            <div id="example-box">
                <figure>
                    <img class="example-img" src="assets/illustration.png" alt="rjudge.png">
                    <figcaption style="text-align: center;">Figure 1. Illustration of R-Judge by an example.</figcaption>
                </figure>
            </div>
            <p style="line-height: 150%">
                Large language models (LLMs) have exhibited great potential in autonomously completing tasks across 
                real-world applications. Despite this, these LLM agents introduce unexpected safety risks when 
                operating in interactive environments. Instead of centering on LLM-generated content safety 
                in most prior studies, this work addresses the imperative need for benchmarking the behavioral 
                safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to 
                evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction 
                records. <br>
            </p>
            <p>
                The figure illustrates R-Judge: The upper left part is an example in the dataset, 
                i.e., a record of an agent interacting with the user and environment. 
                The lower left part is human annotation involving a binary safety label and 
                high-quality risk description. Following the arrow, we can see the serial evaluation paradigm 
                with two tests: given record and task instruction, 
                LLMs are asked to generate an analysis and a label. 
                An automatic evaluator compares the analysis with the ground truth risk description 
                to assess the effectiveness of risk identification. 
                And correct generated labels are counted to evaluate the performance of safety judgment.
            </p>

        </div>

        <!-- <hr> -->
        <div class="section">
            <div id="dataset" class="anchor"></div>
            <h1>R-Judge Dataset</h1>
            <p>
                R-Judge comprises 162 records of multi-turn agent interaction, 
                encompassing 27 key risk scenarios among 7 application categories and 10 risk types. 
                It incorporates human consensus on safety with annotated safety labels and high-quality risk descriptions. 
            </p>
            <div id="example-box">
                <figure>
                    <img class="example-img" src="assets/datasetintro.png" style="width: 80%" alt="datasetintro.png" />
                    <figcaption style="text-align: center;">Figure 2. Dataset distribution of R-Judge.</figcaption>
                </figure>
            </div>
            <div id="example-box">
                
            </div>
            <p>
                For more details, you can explore the datatset and check the visualizations here:
                <a class="ext-link" href="https://rjudgebench.github.io/explore.html">Explore</a> and
                <a class="ext-link" href="https://rjudgebench.github.io/visualize.html">Visualizations</a>.
            </p>
            <p>
                Our dataset is distributed under the
                <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="blank" class="ext-link">
                    CC BY-NC-SA (Attribution-NonCommercial-ShareAlike)
                </a>
                license. You can check out our
                <a href="https://github.com/Lordog/R-Judge" class="ext-link" target="blank">github
                    repository</a> to download the dataset .
            </p>
        </div>

        <!-- <hr> -->
        <div class="section">
            <div id="method" class="anchor"></div>
            <h1>Evaluation Recipes</h1>
            <p>
            Based on the dataset, we evaluate the effectiveness of public LLMs as 
            agent safety monitors. As monitors for agent safety demand 
            both risk identification and safety judgment of LLM capabilities, 
            we design two recipes to evaluate LLMs' proficiency in 
            identifying and judging safety risks given agent interaction records.
            </p>
            <p>
                As shown in the Figure, the two evaluation recipes are conducted in a serial pipeline. 
                First, LLMs are demonstrated with the record as input and asked to analyze whether 
                the agent actions in the record are safe, formalized as:
            </p>
            $$p_{\theta}(R) \to analysis$$

            <p>
            Then, LLMs are asked to output 'unsafe' or 'safe' based on record and analysis in the first step, 
            formalized as:
            </p>
                $$p_{\theta} (R, analysis) \to label$$
            <p>
                The output analysis in the first step and the output label 
                in the second step are taken respectively for the two evaluation recipes, 
                with human annotation as ground truth.
            </p>
            <div id="example-box">
                <img class="example-img" src="assets/illustration.png" alt="rjudge.png">
            </div>
            <p>
                <span style="font-weight: 700;">1. Label Score for Safety Judgment. </span>
                To evaluate the ability of LLMs to make safety judgments, 
                a label-based test compares LLM-generated binary safety labels with truth labels 
                from the consensus of human annotators. 
            </p>
            <p>
                <span style="font-weight: 700;">2. Pairwise Evaluation for Risk Identification. </span>
             To evaluate the effectiveness of LLMs in identifying safety risks, 
             an open-ended test utilizes GPT-4 as an automatic evaluator to assess 
             the open-ended model-generated analysis on unsafe cases. 
            </p>

            <p>
                The results of other baselines and recent work are reported at the <a
                    href="https://rjudgebench.github.io/leaderboard.html" class="ext-link" target="blank">Leaderboard</a>
                page.
            </p>
        </div>

        <!-- <hr> -->
        <div class="section">
            <div id="paper" class="anchor"></div>
            <h1>Paper</h1>
            <p>
                <papername>
                    R-Judge: Benchmarking Safety Risk Awareness for LLM Agents
                </papername><br>
                Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, 
                Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu<br>
                <a href="https://arxiv.org/abs/2401.10019" class="ext-link" target="blank">Paper</a> /
                <a href="https://arxiv.org/pdf/2401.10019.pdf" class="ext-link"
                    target="blank">PDF</a> /
                <a href="https://github.com/Lordog/R-Judge" class="ext-link" target="blank">Code</a>
            </p>
        </div>

        <!-- <hr> -->
        <div class="section">
            <div id="code" class="anchor"></div>
            <h1>Code</h1>
            <p>
                View on the <a href="https://github.com/Lordog/R-Judge" class="ext-link" target="blank">github
                    repository</a>.
            </p>
        </div>

        <!-- <hr> -->
        <div class="section">
            <div id="citation" class="anchor"></div>
            <h1>Citation</h1>
            <p>
                If the paper, codes, or the dataset inspire you, please cite us:
            </p>
            <pre class="bibtax">
@article{yuan2024rjudge,
  title={R-Judge: Benchmarking Safety Risk Awareness for LLM Agents},
  author={Tongxin Yuan and Zhiwei He and Lingzhong Dong and Yiming Wang and Ruijie Zhao and Tian Xia and Lizhen Xu and Binglin Zhou and Fangqi Li and Zhuosheng Zhang and Rui Wang and Gongshen Liu},
  journal={arXiv preprint arXiv:2401.10019},
  year={2024}
}</pre>
            <br>
        </div>

        <!-- Authors -->
        <!--  
        <div class="section" id="authors" style="align-content: center">
            <h1>Authors</h1>
            <div style="text-align: center; width: 100%; padding-top: 1em;">
                <div class="profile">
                    <a href="https://lordog.github.io/">
                        <img class="profile-img" src="./img/ytx.jpg">
                        <p>Tongxin Yuan<sup>1,3</sup></p>
                    </a>
        </div>
        -->

        <!-- Contact -->
        <div class="section" id="contact">
            <h1>Contact</h1>
            <p>
                Questions about R-Judge, or want to get in touch? Contact Tongxin Yuan at
                <a href="mailto:teenyuan@sjtu.edu.cn">Email</a>, or open
                up an issue on
                <a href="https://github.com/Lordog/R-Judge/issues" target="blank" class="ext-link">Github</a>.
            </p>
        </div>

        <!-- Affiliation  -->
        <div class="section" style="table-layout: auto;">
            <center>
                <table>
                    <tbody>
                        <tr>
                            <td>
                                <a href="https://www.sjtu.edu.cn//" target="_blank" rel="external">
                                    <img class="center-block" src="assets/sjtu.png"
                                        style="height:6em; max-width: 100%;"></a>
                            </td>
                        </tr>
                    </tbody>
                </table><br>
            </center>
        </div>


    </div>
    <!-- footer -->
    <div id="footer" style="font-size: 12pt">
        © 2024 SJTU. Website developed based on the ScienceQA template.
    </div>
</body>

</html>
